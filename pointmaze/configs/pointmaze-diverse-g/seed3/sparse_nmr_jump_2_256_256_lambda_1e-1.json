{
    "env": {
        "name": "PointMaze_Large_Diverse_G-v3",
        "normalize": true,
        "use_mega": false,
        "mega_sample_n": 10,
        "use_nmr": true,
        "nmr_jump" : true,
        "nmr_jump_cnt": 2,
        "nmr_length": 20,
        "use_original_reward": false,
        "non_terminal_reward": -1.0,
        "terminal_reward": 0.0,
        "custom_map": [
            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
            [1, "r", "g", "g", "g", 1, "g", "g", "g", "g", "g", 1],
            [1, "g", 1, 1, "g", 1, "g", 1, "g", 1, "g", 1],
            [1, "g", "g", "g", "g", "g", "g", 1, "g", "g", "g", 1],
            [1, "g", 1, 1, 1, 1, "g", 1, 1, 1, "g", 1],
            [1, "g", "g", 1, "g", 1, "g", "g", "g", "g", "g", 1],
            [1, 1, "g", 1, "g", 1, "g", 1, "g", 1, 1, 1],
            [1, "g", "g", 1, "g", "g", "g", 1, "g", "g", "g", 1],
            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
        ]
    },
    "bc": {
        "experiment_name": "pointmaze_256_256_100epochs_loss_3",
        "policy_file_save_name": "bc_checkpoint",
        "policy_after_value_head_trained_file_save_name": "bc_checkpoint_after_value_head_trained",
        "data_cache_dir": "cache",
        "dataset_minari_id": "pointmaze-large-dense-v2",
        "seed": 24,
        "train_epochs": 100,
        "batch_size": 256,
        "l2_weight": 1e-4,
        "ent_weight": 1e-2,
        "loss_threshold": 2.0,
        "dataset_split": [0.96, 0.02, 0.02]
    },
    "rl_bc": {
        "experiment_name": "iter_1/pointmaze_large_diverse_g_nmr_jump_2_256_256_1e7steps_16envs_lambda_1e-1_loss_3",
        "seed": 26,
        "seed_for_load_algo": 27,
        "net_arch": [256, 256],
        "batch_size": 256,
        "ent_coef": 1e-4,
        "lr": 1e-4,
        "gamma": 0.98,
        "gae_lambda": 0.92,
        "activate_value_head_train_steps": 1e6,
        "kl_with_bc_model_coef": 1e-1,
        "train_steps": 1e7,
        "rollout_process_num": 16,
        "evaluate_process_num": 16,
        "callback_process_num": 16,
        "evaluate_nums_in_evaluation": 30,
        "__evaluate_nums_in_evaluation": "使用evaluate_nums_in_evaluation * evaluate_process_num个episodes评估策略",
        "evaluate_nums_in_callback": 3,
        "__evaluate_nums_in_callback": "使用evaluate_nums_in_callback * evaluate_process_num个episodes评估策略",
        "//n_steps": "采样时每个环境采样的step数，PPO每次训练收集的数据量是n_steps * num_envs",
        "n_steps": 256,
        "//n_epochs": "采样的数据在训练中重复使用的次数",
        "n_epochs": 5,
        "__eval_freq": "多少次env.step()评估一次，此处设置为1000，因为VecEnv有72个并行环境，所以实际相当于72*1000次step，评估一次",
        "eval_freq": 1024
    },
    "rl": {
        "experiment_name": "iter_1/pointmaze_large_diverse_g_nmr_jump_2_256_256_2e7steps_16_normal_envs_loss_3_singleRL",
        "seed": 29,
        "train_steps": 2e7,
        "batch_size": 256,
        "ent_coef": 0.0,
        "max_grad_norm": 0.5,
        "learning_rate": 3e-4,
        "vf_coef": 0.5,
        "clip_range": 0.2,
        "rollout_process_num": 16,
        "evaluate_process_num": 16,
        "callback_process_num": 16,
        "evaluate_nums_in_evaluation": 30,
        "__evaluate_nums_in_evaluation": "使用evaluate_nums_in_evaluation * evaluate_process_num个episodes评估策略",
        "evaluate_nums_in_callback": 3,
        "__evaluate_nums_in_callback": "使用evaluate_nums_in_callback * evaluate_process_num个episodes评估策略",
        "//n_steps": "采样时每个环境采样的step数，PPO每次训练收集的数据量是n_steps * num_envs",
        "n_steps": 256,
        "//n_epochs": "采样的数据在训练中重复使用的次数",
        "n_epochs": 5,
        "__eval_freq": "多少次env.step()评估一次，此处设置为1000，因为VecEnv有72个并行环境，所以实际相当于72*1000次step，评估一次",
        "eval_freq": 1024
    }
}
